{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c132d78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.3.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\22anj\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading pyarrow-18.0.0-cp311-cp311-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/25.1 MB 2.4 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 1.3/25.1 MB 3.5 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.4/25.1 MB 3.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.1/25.1 MB 3.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 4.2/25.1 MB 3.9 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 5.0/25.1 MB 4.0 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 6.0/25.1 MB 4.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 7.1/25.1 MB 4.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 8.1/25.1 MB 4.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.2/25.1 MB 4.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 10.5/25.1 MB 4.5 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 11.3/25.1 MB 4.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.6/25.1 MB 4.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.6/25.1 MB 4.5 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.7/25.1 MB 4.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.7/25.1 MB 4.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.8/25.1 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.8/25.1 MB 4.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.6/25.1 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.9/25.1 MB 4.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.0/25.1 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.0/25.1 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.1/25.1 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.1/25.1 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, tqdm, requests, pyarrow, fsspec, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 11.0.0\n",
      "    Uninstalling pyarrow-11.0.0:\n",
      "      Successfully uninstalled pyarrow-11.0.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.3.0\n",
      "    Uninstalling fsspec-2023.3.0:\n",
      "      Successfully uninstalled fsspec-2023.3.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "Successfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 huggingface-hub-0.26.2 multiprocess-0.70.16 pyarrow-18.0.0 requests-2.32.3 tqdm-4.67.0 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\22anj\\anaconda3\\Lib\\site-packages\\~yarrow'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.41 requires requests_mock, which is not installed.\n",
      "transformers 2.1.1 requires sentencepiece, which is not installed.\n",
      "conda-repo-cli 1.0.41 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires requests==2.28.1, but you have requests 2.32.3 which is incompatible.\n",
      "s3fs 2023.3.0 requires fsspec==2023.3.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04d872e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\22anj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\22anj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed62720da8e74b74a5bebe05cb3a6860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22anj\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\22anj\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4bf6ee7ce346dca81c4a33b3a3fe5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36677a256607465085bfc0593f406678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c575bc99264df6b3fe98ca3892069b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9827297816724faa81401e769941a80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760c761dedee4abd8097b9a71c88908e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9567b4b4c4d43e584b1fe4e91d9b4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.8759\n",
      "SVM F1-Score: 0.8759\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88     12500\n",
      "           1       0.88      0.87      0.88     12500\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n",
      "Random Forest Accuracy: 0.8448\n",
      "Random Forest F1-Score: 0.8448\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.85     12500\n",
      "           1       0.86      0.83      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.85      0.84      0.84     25000\n",
      "weighted avg       0.85      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Ensure nltk resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the IMDB dataset using the datasets library\n",
    "imdb_data = load_dataset('imdb')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = imdb_data['train']\n",
    "test_data = imdb_data['test']\n",
    "\n",
    "# Extract texts and labels\n",
    "X_train = train_data['text']\n",
    "y_train = train_data['label']\n",
    "X_test = test_data['text']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Preprocess text: tokenization and stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "X_train_preprocessed = [preprocess_text(doc) for doc in X_train]\n",
    "X_test_preprocessed = [preprocess_text(doc) for doc in X_test]\n",
    "\n",
    "# Transform text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_preprocessed)\n",
    "X_test_tfidf = vectorizer.transform(X_test_preprocessed)\n",
    "\n",
    "# Train a Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "print(f'SVM Accuracy: {accuracy_svm:.4f}')\n",
    "print(f'SVM F1-Score: {f1_svm:.4f}')\n",
    "print(\"SVM Classification Report:\\n\", classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "print(f'Random Forest Accuracy: {accuracy_rf:.4f}')\n",
    "print(f'Random Forest F1-Score: {f1_rf:.4f}')\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b0e77e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
