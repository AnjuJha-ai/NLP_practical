{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b5e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a471980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\22anj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2002.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9395\n",
      "Confusion Matrix:\n",
      "[[  735    72    12     8     0     9     3   215    30]\n",
      " [   75  1014     3     0     1     2    11   275    19]\n",
      " [   18     5   484     0     4     0     1   222     1]\n",
      " [   19     6     4   143     3     6     1   128    15]\n",
      " [    7     2    12     0   463     0     1   144     5]\n",
      " [   11     7     4     8     6   161     3   327    30]\n",
      " [    5    14     2     0     0     9   100   204     5]\n",
      " [   36   147    38    15    26   131    75 44708   179]\n",
      " [   41    17     1    12     1    35    11   381   605]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.78      0.68      0.72      1084\n",
      "       B-ORG       0.79      0.72      0.76      1400\n",
      "       B-PER       0.86      0.66      0.75       735\n",
      "       I-LOC       0.77      0.44      0.56       325\n",
      "       I-PER       0.92      0.73      0.81       634\n",
      "      I-MISC       0.46      0.29      0.35       557\n",
      "      B-MISC       0.49      0.29      0.37       339\n",
      "           O       0.96      0.99      0.97     45355\n",
      "       I-ORG       0.68      0.55      0.61      1104\n",
      "\n",
      "    accuracy                           0.94     51533\n",
      "   macro avg       0.74      0.59      0.66     51533\n",
      "weighted avg       0.93      0.94      0.93     51533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# resources are downloaded\n",
    "nltk.download('conll2002')\n",
    "\n",
    "# Load the CoNLL-2002 dataset (Spanish NER)\n",
    "train_data = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_data = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
    "\n",
    "# Extract states (entity tags) and observations (words)\n",
    "def get_tags(data):\n",
    "    tags = set()\n",
    "    for sentence in data:\n",
    "        for word, pos, tag in sentence:\n",
    "            tags.add(tag)\n",
    "    return list(tags)\n",
    "\n",
    "def get_words(data):\n",
    "    words = set()\n",
    "    for sentence in data:\n",
    "        for word, pos, tag in sentence:\n",
    "            words.add(word.lower())\n",
    "    return list(words)\n",
    "\n",
    "tags = get_tags(train_data)\n",
    "words = get_words(train_data)\n",
    "\n",
    "# Initialize probabilities\n",
    "transition_counts = defaultdict(Counter)\n",
    "emission_counts = defaultdict(Counter)\n",
    "start_counts = Counter()\n",
    "\n",
    "# Calculate counts for transitions, emissions, and start states\n",
    "for sentence in train_data:\n",
    "    prev_tag = '<s>'\n",
    "    for word, pos, tag in sentence:\n",
    "        start_counts[tag] += (prev_tag == '<s>')\n",
    "        transition_counts[prev_tag][tag] += 1\n",
    "        emission_counts[tag][word.lower()] += 1\n",
    "        prev_tag = tag\n",
    "    transition_counts[prev_tag]['</s>'] += 1\n",
    "\n",
    "# Convert counts to probabilities\n",
    "def normalize(counter):\n",
    "    total = sum(counter.values())\n",
    "    return {key: value / total for key, value in counter.items()}\n",
    "\n",
    "start_probs = normalize(start_counts)\n",
    "transition_probs = {prev_tag: normalize(tags_counts) for prev_tag, tags_counts in transition_counts.items()}\n",
    "emission_probs = {tag: normalize(words_counts) for tag, words_counts in emission_counts.items()}\n",
    "\n",
    "# Define Viterbi algorithm for decoding\n",
    "def viterbi(sequence, states, start_prob, transition_prob, emission_prob):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    for state in states:\n",
    "        V[0][state] = start_prob.get(state, 0) * emission_prob.get(state, {}).get(sequence[0], 1e-6)\n",
    "        path[state] = [state]\n",
    "\n",
    "    for t in range(1, len(sequence)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    "\n",
    "        for state in states:\n",
    "            (prob, best_prev_state) = max(\n",
    "                (V[t - 1][prev_state] * transition_prob.get(prev_state, {}).get(state, 1e-6) *\n",
    "                 emission_prob.get(state, {}).get(sequence[t], 1e-6), prev_state)\n",
    "                for prev_state in states\n",
    "            )\n",
    "\n",
    "            V[t][state] = prob\n",
    "            newpath[state] = path[best_prev_state] + [state]\n",
    "\n",
    "        path = newpath\n",
    "\n",
    "    (prob, best_final_state) = max((V[-1][state], state) for state in states)\n",
    "    return path[best_final_state]\n",
    "\n",
    "# Evaluate the HMM model on the test dataset\n",
    "def evaluate_model(test_data, states, transition_probs, emission_probs):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for sentence in test_data:\n",
    "        words = [word.lower() for word, pos, tag in sentence]\n",
    "        true_tags = [tag for word, pos, tag in sentence]\n",
    "        predicted_tags = viterbi(words, states, start_probs, transition_probs, emission_probs)\n",
    "        y_true.extend(true_tags)\n",
    "        y_pred.extend(predicted_tags)\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Get true and predicted tags\n",
    "y_true, y_pred = evaluate_model(test_data, tags, transition_probs, emission_probs)\n",
    "\n",
    "# Report accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Generate and display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred, labels=tags)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Display classification report\n",
    "print(classification_report(y_true, y_pred, labels=tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9131bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
