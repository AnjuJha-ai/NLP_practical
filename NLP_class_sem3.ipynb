{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3314c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa3972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"\"\"...Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "...And the first lesson of all was the basic trust that he could learn.\n",
    "...It's shocking to find how many people do not believe they can learn,\n",
    "...and how many more believe learning to be difficult.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5677fb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"...Muad'Dib learned rapidly because his first training was in how to learn.\",\n",
       " '...And the first lesson of all was the basic trust that he could learn.',\n",
       " \"...It's shocking to find how many people do not believe they can learn,\\n...and how many more believe learning to be difficult.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d168a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\22anj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2880a018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['...',\n",
       " \"Muad'Dib\",\n",
       " 'learned',\n",
       " 'rapidly',\n",
       " 'because',\n",
       " 'his',\n",
       " 'first',\n",
       " 'training',\n",
       " 'was',\n",
       " 'in',\n",
       " 'how',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.',\n",
       " '...',\n",
       " 'And',\n",
       " 'the',\n",
       " 'first',\n",
       " 'lesson',\n",
       " 'of',\n",
       " 'all',\n",
       " 'was',\n",
       " 'the',\n",
       " 'basic',\n",
       " 'trust',\n",
       " 'that',\n",
       " 'he',\n",
       " 'could',\n",
       " 'learn',\n",
       " '.',\n",
       " '...',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'shocking',\n",
       " 'to',\n",
       " 'find',\n",
       " 'how',\n",
       " 'many',\n",
       " 'people',\n",
       " 'do',\n",
       " 'not',\n",
       " 'believe',\n",
       " 'they',\n",
       " 'can',\n",
       " 'learn',\n",
       " ',',\n",
       " '...',\n",
       " 'and',\n",
       " 'how',\n",
       " 'many',\n",
       " 'more',\n",
       " 'believe',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'be',\n",
       " 'difficult',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c20ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\22anj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d64b541b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worf_quote=\"Sir , I protest. I am not a merry man!\"\n",
    "words_in_quote=word_tokenize(worf_quote)\n",
    "words_in_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2f5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c942f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'protest', '.', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list=[]\n",
    "for word in words_in_quote:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)\n",
    "filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d148b6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'protest', '.', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list=[word for word in words_in_quote if word.casefold() not in stop_words]\n",
    "filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5ce3442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\22anj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d39313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\22anj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87b56434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\22anj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d15790ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\22anj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2fe3645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'humans', 'and', 'computers', 'using', 'natural', 'language', '.']\n",
      "['Natural', 'Language', 'Processing', 'subfield', 'artificial', 'intelligence', 'focuses', 'interaction', 'humans', 'computers', 'using', 'natural', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing is a subfield of artificial intelligence that focuses on the interaction between humans and computers using natural language.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"Natural Language Processing is a subfield of artificial intelligence that focuses on the interaction between humans and computers using natural language.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87e7c354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'subfield', 'artificial', 'intelligence', 'focus', 'interaction', 'human', 'computer', 'using', 'natural', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = \"Natural Language Processing is a subfield of artificial intelligence that focuses on the interaction between humans and computers using natural language.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "print(lemmatized_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aec3af76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.268, 'pos': 0.732, 'compound': 0.928}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "text = \"I love natural language processing! It's so interesting and useful.\"\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentiment_scores = analyzer.polarity_scores(text)\n",
    "print(sentiment_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cde346ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  born/VBD\n",
      "  (PERSON Hawaii/NNP)\n",
      "  served/VBD\n",
      "  44th/CD\n",
      "  President/NNP\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  2009/CD\n",
      "  2017/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ne_chunk\n",
    "\n",
    "text = \"Barack Obama was born in Hawaii and served as the 44th President of the United States from 2009 to 2017.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "pos_tags = nltk.pos_tag(lemmatized_tokens)\n",
    "\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "print(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4605ec16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'jump', 'better']\n"
     ]
    }
   ],
   "source": [
    "#Stemming using NLTK's Porter Stemmer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"jumping\", \"better\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)  # Output: ['runn', 'jump', 'better']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ee0f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'jump', 'better']\n"
     ]
    }
   ],
   "source": [
    "#Stemming using NLTK's Snowball Stemmer\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "words = [\"running\", \"jumping\", \"better\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)  # Output: ['run', 'jump', 'better']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03489730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'jump', 'better']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization using NLTK's WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"jumping\", \"better\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word,pos='v') for word in words]\n",
    "\n",
    "print(lemmatized_words)  # Output: ['run', 'jump', 'good']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5d527dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b355a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli import download\n",
    "\n",
    "download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4d4a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'jump', 'well']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization using spaCy*\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "words = [\"running\", \"jumping\", \"better\"]\n",
    "lemmatized_words = [token.lemma_ for token in nlp(' '.join(words))]\n",
    "\n",
    "print(lemmatized_words)  # Output: ['run', 'jump', 'good']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f89e879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  people/NNS\n",
      "  n't/RB\n",
      "  know/VB\n",
      "  swimming/VBG\n",
      "  ,/,\n",
      "  come/VB\n",
      "  survive/JJ\n",
      "  flood/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "s=\"There are some people who don't know swimming, how come they survive in flood.\"\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ne_chunk\n",
    "\n",
    "tokens = word_tokenize(s)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "pos_tags = nltk.pos_tag(lemmatized_tokens)\n",
    "\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "print(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12b0e73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'some', 'people', 'who', 'do', \"n't\", 'know', 'swimming', ',', 'how', 'come', 'they', 'survive', 'in', 'flood', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "s=\"There are some people who don't know swimming, how come they survive in flood.\"\n",
    "tokens = word_tokenize(s)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1764b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'some', 'people', 'who', 'do', \"n't\", 'know', 'swimming', ',', 'how', 'come', 'they', 'survive', 'in', 'flood', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b659f817",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SnowballStemmer\n\u001b[0;32m      3\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m SnowballStemmer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m stemmed_words \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(tokens) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(stemmed_words)\n",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SnowballStemmer\n\u001b[0;32m      3\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m SnowballStemmer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m stemmed_words \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(tokens) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(stemmed_words)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\stem\\snowball.py:1417\u001b[0m, in \u001b[0;36mEnglishStemmer.stem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstem\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03m    Stem an English word and return the stemmed form.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1415\u001b[0m \n\u001b[0;32m   1416\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1417\u001b[0m     word \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m word\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmed_words = [stemmer.stem(tokens) for token in tokens]\n",
    "\n",
    "print(stemmed_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17d6cc05",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m----> 2\u001b[0m stemmed_words \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(tokens) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(stemmed_words)\n",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m----> 2\u001b[0m stemmed_words \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(tokens) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(stemmed_words)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\stem\\porter.py:658\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[1;34m(self, word, to_lowercase)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstem\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, to_lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    655\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;124;03m    :param to_lowercase: if `to_lowercase=True` the word always lowercase\u001b[39;00m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 658\u001b[0m     stem \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mif\u001b[39;00m to_lowercase \u001b[38;5;28;01melse\u001b[39;00m word\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNLTK_EXTENSIONS \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool[stem]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(tokens) for token in tokens]\n",
    "\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef7ae499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15c84e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "S=\"stars lies in the sky\"\n",
    "R=\"Moon lies in the sky\"\n",
    "tokens=S+ \" \"+ R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e64057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stars lies in the sky Moon lies in the sky\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27f3fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=word_tokenize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b5c18e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stars', 'lies', 'in', 'the', 'sky', 'Moon', 'lies', 'in', 'the', 'sky']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "697e9e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82570933",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\"stars lies in the sky\",\"Moon lies in the sky\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdf2a96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 1 1]\n",
      " [1 1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vector=CountVectorizer()\n",
    "x=vector.fit_transform(corpus)\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7bc0144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in' 'lies' 'moon' 'sky' 'stars' 'the']\n"
     ]
    }
   ],
   "source": [
    "print(vector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f71713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 1 1 1 1 0 2]\n",
      " [1 0 1 1 0 0 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"The lazy dog sleeps all day\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7a21555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into a document-term matrix\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the document-term matrix as an array\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caeaa55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample data (replace with your own data)\n",
    "text_data = [\n",
    "    \"This is a positive review.\",\n",
    "    \"I really liked this product.\",\n",
    "    \"This product is terrible.\",\n",
    "    \"I would not recommend this.\",\n",
    "    \"This is a great product.\"\n",
    "]\n",
    "labels = [1, 1, 0, 0, 1]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Create BoW representation\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7316c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n",
      "  (0, 1)\t0.4387767428592343\n",
      "  (0, 2)\t0.5419765697264572\n",
      "  (0, 6)\t0.35872873824808993\n",
      "  (0, 3)\t0.4387767428592343\n",
      "  (0, 8)\t0.4387767428592343\n",
      "  (1, 5)\t0.8532257361452786\n",
      "  (1, 1)\t0.2723014675233404\n",
      "  (1, 6)\t0.22262429232510395\n",
      "  (1, 3)\t0.2723014675233404\n",
      "  (1, 8)\t0.2723014675233404\n",
      "  (2, 4)\t0.5528053199908667\n",
      "  (2, 7)\t0.5528053199908667\n",
      "  (2, 0)\t0.5528053199908667\n",
      "  (2, 6)\t0.2884767487500274\n",
      "  (3, 1)\t0.4387767428592343\n",
      "  (3, 2)\t0.5419765697264572\n",
      "  (3, 6)\t0.35872873824808993\n",
      "  (3, 3)\t0.4387767428592343\n",
      "  (3, 8)\t0.4387767428592343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# BoW\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)  # corpus is a list of documents\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b88c5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown' 'day' 'dog' 'fox' 'jumps' 'lazy' 'quick' 'sleeps']\n",
      "[[0.44665616 0.         0.31779954 0.44665616 0.44665616 0.31779954\n",
      "  0.44665616 0.        ]\n",
      " [0.         0.57615236 0.40993715 0.         0.         0.40993715\n",
      "  0.         0.57615236]]\n",
      "  (0, 2)\t0.31779953783628945\n",
      "  (0, 5)\t0.31779953783628945\n",
      "  (0, 4)\t0.4466561618018052\n",
      "  (0, 3)\t0.4466561618018052\n",
      "  (0, 0)\t0.4466561618018052\n",
      "  (0, 6)\t0.4466561618018052\n",
      "  (1, 1)\t0.5761523551647353\n",
      "  (1, 7)\t0.5761523551647353\n",
      "  (1, 2)\t0.40993714596036396\n",
      "  (1, 5)\t0.40993714596036396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sample text data\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"The lazy dog sleeps all day\"\n",
    "]\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Preprocess and vectorize text\n",
    "corpus_preprocessed = [preprocess(doc) for doc in corpus]\n",
    "X = vectorizer.fit_transform(corpus_preprocessed)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7568e54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown' 'dog' 'fox' 'jumps' 'lazy' 'over' 'quick' 'the']\n",
      "[[0.30151134 0.30151134 0.30151134 0.30151134 0.30151134 0.30151134\n",
      "  0.30151134 0.60302269]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = [\"The quick brown fox jumps over the lazy dog\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(text)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c723c5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all' 'brown' 'day' 'dog' 'fox' 'jumps' 'lazy' 'over' 'quick' 'sleeps'\n",
      " 'the']\n",
      "[[0.         0.35272845 0.         0.25096919 0.35272845 0.35272845\n",
      "  0.25096919 0.35272845 0.35272845 0.         0.50193839]\n",
      " [0.47042643 0.         0.47042643 0.33471228 0.         0.\n",
      "  0.33471228 0.         0.         0.47042643 0.33471228]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"The lazy dog sleeps all day\"\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into a document-term matrix\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the TF-IDF values\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "170a4b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all' 'and' 'brown' 'comes' 'day' 'dog' 'everyday' 'expertise' 'fox'\n",
      " 'graduate' 'in' 'jumps' 'late' 'lazy' 'learning' 'machine' 'over'\n",
      " 'python' 'quick' 'robin' 'sleeps' 'strong' 'the' 'with']\n",
      "\n",
      "\n",
      "[[0.         0.2795495  0.3545731  0.         0.         0.2795495\n",
      "  0.         0.         0.3545731  0.         0.         0.3545731\n",
      "  0.         0.2795495  0.         0.         0.3545731  0.\n",
      "  0.3545731  0.         0.         0.         0.3700619  0.        ]\n",
      " [0.         0.26436658 0.         0.         0.         0.\n",
      "  0.         0.33531548 0.         0.33531548 0.33531548 0.\n",
      "  0.         0.         0.33531548 0.33531548 0.         0.33531548\n",
      "  0.         0.         0.         0.33531548 0.17498153 0.33531548]\n",
      " [0.47059455 0.         0.         0.         0.47059455 0.37102215\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.37102215 0.         0.         0.         0.\n",
      "  0.         0.         0.47059455 0.         0.24557576 0.        ]\n",
      " [0.         0.         0.         0.48380259 0.         0.\n",
      "  0.48380259 0.         0.         0.         0.         0.\n",
      "  0.48380259 0.         0.         0.         0.         0.\n",
      "  0.         0.48380259 0.         0.         0.25246826 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = [\"The quick brown fox and jumps the over lazy dog\",\n",
    "        \"the Graduate with strong expertise in python and machine learning\",\n",
    "       \"the lazy dog sleeps all day\",\n",
    "       \"the robin comes late everyday\"]\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(\"\\n\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3028a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ea743f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "line=\"hello hello my name is mita\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2484c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'hello', 'my', 'name', 'is', 'mita']\n"
     ]
    }
   ],
   "source": [
    "k=word_tokenize(line)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a385aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hellohello', 'hellomy', 'myname', 'nameis', 'ismita']\n"
     ]
    }
   ],
   "source": [
    "tem=[]\n",
    "for i in range (len(k)-1):\n",
    "    h=(k[i]+k[i+1])\n",
    "    tem.append(h)\n",
    "    \n",
    "print(tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0879a4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.2, 0.2, 0.2, 0.2]\n"
     ]
    }
   ],
   "source": [
    "u=[]\n",
    "for i in tem:\n",
    "    i=1\n",
    "    prob=i/len(set(k))\n",
    "    u.append(prob)\n",
    "    \n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d364a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'hellohello': 0.2}, {'hellomy': 0.2}, {'myname': 0.2}, {'nameis': 0.2}, {'ismita': 0.2}]\n"
     ]
    }
   ],
   "source": [
    "jk=[]\n",
    "for i,j in zip(tem,u):\n",
    "    dic={i:j}\n",
    "    jk.append(dic)\n",
    "print(jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a281ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.count(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cb1764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('hello', 'my'): 1, ('my', 'name'): 1, ('name', 'is'): 1, ('is', 'lela'): 1}\n",
      "('hello', 'my') 1\n",
      "('my', 'name') 1\n",
      "('name', 'is') 1\n",
      "('is', 'lela') 1\n"
     ]
    }
   ],
   "source": [
    "def count_n(text,n):\n",
    "    words=text.lower().split()\n",
    "    counts={}\n",
    "    \n",
    "    for i in range(len(words)-n+1):\n",
    "        ngram=tuple(words[i:i+n])\n",
    "        counts[ngram]=counts.get(ngram,0)+1\n",
    "    return counts\n",
    "\n",
    "text=\"hello my name is lela\"\n",
    "bigram_counts=count_n(text,2)\n",
    "print(bigram_counts)\n",
    "\n",
    "\n",
    "for bigram,count in bigram_counts.items():\n",
    "    print(bigram,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20d31542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob(n_gram):\n",
    "    total_counts=sum(n_gram.values())\n",
    "    proablities={'n_gram':count/total_counts for ngram,count in n_gram.items()}\n",
    "    \n",
    "    return proablities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df3c89cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "proablities = calculate_prob(bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e43f867b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_gram': 0.25}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proablities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094118a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
